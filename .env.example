# LLM Configuration
# Provider priority: GeminiDirect PRIMARY → OpenAI (GPT-4.1-mini) → VertexLlama → NVIDIA → Groq → OpenRouter → Ollama
# GCP DSQ Tier 1: 300 RPM nominal, ~30 RPM actual. Rate limiter: 1.5 req/s (90 RPM) + exponential backoff.
NVIDIA_API_KEY=your-nvidia-api-key-here
NVIDIA_MODEL=deepseek-ai/deepseek-v3.1
NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1
USE_OLLAMA=false
# Tavily: web search + company lookup (key rotation across 2 keys)
TAVILY_ENABLED=false
TAVILY_API_KEYS=your-tavily-key-1,your-tavily-key-2
USE_DDG_FALLBACK=true
OLLAMA_MODEL=llama3.2:3b
OLLAMA_BASE_URL=http://localhost:11434
# Ollama dual-model strategy (MX550 2GB VRAM — do NOT use 7B+ models)
OLLAMA_GEN_MODEL=phi3.5-custom:latest
OLLAMA_TOOL_MODEL=llama3.2:3b
# SearXNG: docker run -d -p 8888:8080 searxng/searxng:latest
SEARXNG_ENABLED=true
SEARXNG_URL=http://localhost:8888
OFFLINE_MODE=false
GEMINI_API_KEY=your-gemini-api-key-here
# Primary model: 2.0-flash ($0.10/$0.40 per 1M in/out) — high Vertex AI DSQ capacity, 300 RPM Tier 1
# Lite model: 2.5-flash-lite ($0.10/$0.40 per 1M in/out) — separate DSQ pool for classification
# Using TWO models splits load across DSQ pools → effectively doubles rate limit headroom.
GEMINI_MODEL=gemini-2.0-flash
GEMINI_LITE_MODEL=gemini-2.5-flash-lite

# Vertex AI Express Mode (free tier fallback — 10 RPM, 90 days)
VERTEX_EXPRESS_API_KEY=your-vertex-express-api-key-here

# Full Vertex AI ($300 GCP credits — 60+ RPM, much faster than Express)
GCP_PROJECT_ID=your-gcp-project-id
GCP_SERVICE_ACCOUNT_FILE=your-service-account-file.json
GCP_VERTEX_LOCATION=us-central1

# Vertex AI API Service — partner models (same GCP project, same service account)
# DeepSeek V3.2: best reasoning + tool calling, ~$0.07/$0.14 per 1M in/out
# Llama 4 Scout: fastest tool-calling, ~$0.04/$0.08 per 1M in/out
# Set to "" to disable either model.
VERTEX_DEEPSEEK_MODEL=
VERTEX_LLAMA_MODEL=meta/llama-3.3-70b-instruct-maas

GROQ_API_KEY=your-groq-api-key-here
# General synthesis/generation — llama-3.3-70b-versatile (higher rate limit than qwen3)
GROQ_MODEL=llama-3.3-70b-versatile
# Tool/function calling (confirmed tool use support on Groq)
GROQ_TOOL_MODEL=llama-3.3-70b-versatile
OPENROUTER_API_KEY=your-openrouter-api-key-here
OPENROUTER_MODEL=google/gemini-2.5-flash

# Embedding Configuration
# Primary: NVIDIA nv-embedqa-e5-v5 (1024-dim, best discrimination — 40% sharper than bge-large)
# Fallback: BAAI/bge-large-en-v1.5 (1024-dim, same dimensions, HF API or local)
HF_API_KEY=your-huggingface-api-key-here
EMBEDDING_MODEL=nvidia/nv-embedqa-e5-v5
LOCAL_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
# Provider priority: "nvidia" = NVIDIA NIM API (best), "api" = HuggingFace GPUs, "local" = CPU/GPU
EMBEDDING_PROVIDER=nvidia

# ── Trend Engine Pipeline ──
# UMAP: Dimensionality reduction (1024→5 dim for HDBSCAN)
UMAP_N_COMPONENTS=5
UMAP_N_NEIGHBORS=15
UMAP_MIN_DIST=0.0
UMAP_METRIC=cosine

# HDBSCAN: Density-based clustering (auto-tunes density thresholds)
HDBSCAN_MIN_CLUSTER_SIZE=5
HDBSCAN_MIN_SAMPLES=3
HDBSCAN_CLUSTER_SELECTION=eom

# Deduplication: MinHash LSH (0.25 = aggressive, catches ~25% shared content)
DEDUP_THRESHOLD=0.25
DEDUP_NUM_PERM=128

# Semantic Deduplication: Embedding similarity
# Recalibrated for nv-embedqa-e5-v5 (true dupes ~0.95+, related ~0.68-0.72)
SEMANTIC_DEDUP_THRESHOLD=0.88

# Entity extraction: spaCy NER model
SPACY_MODEL=en_core_web_sm

# Engine settings
ENGINE_MAX_DEPTH=2
# Max parallel LLM calls. DSQ actual ~30 RPM, rate limiter 1.5 req/s → 2 concurrent max.
# Was 6 (429 storms), then 3 (still 429 after 12 calls). 2 is safe.
ENGINE_MAX_CONCURRENT_LLM=2

# Search API (supports multiple keys for rotation — comma-separated)
# TAVILY_API_KEYS=

# News APIs (for trend detection)
NEWSAPI_ORG_KEY=your-newsapi-org-key-here
RAPIDAPI_KEY=your-rapidapi-key-here
GNEWS_API_KEY=your-gnews-api-key-here
MEDIASTACK_API_KEY=your-mediastack-api-key-here
THENEWSAPI_KEY=your-thenewsapi-key-here

# Email Finder APIs
APOLLO_API_KEY=your-apollo-api-key-here
HUNTER_API_KEY=your-hunter-api-key-here

# Settings
COUNTRY=India
MAX_TRENDS=8
MAX_COMPANIES_PER_TREND=5
MAX_CONTACTS_PER_COMPANY=3
EMAIL_CONFIDENCE_THRESHOLD=70
MOCK_MODE=false
SHOW_TOOLTIPS=true
DATABASE_URL=sqlite+aiosqlite:///./leads.db

# === Pipeline Tuning ===
# Adjust these to fine-tune trend detection quality. Run calibrate.py to find optimal values.

# RSS Throughput (more articles = better clustering)
RSS_MAX_PER_SOURCE=25
RSS_HOURS_AGO=72

# Event Classification (Tier 2 LLM for ambiguous articles)
EVENT_MAX_LLM_CALLS=20

# Coherence Validation (cluster quality gates)
# COHERENCE_MIN: clusters below this get flagged for review
# COHERENCE_REJECT: clusters below this get dissolved to noise
# MERGE_THRESHOLD: centroid similarity above this merges clusters
COHERENCE_MIN=0.44
# Raised from 0.35 → 0.44: stress test (6 runs, ρ=+0.60) shows higher = better composite
COHERENCE_REJECT=0.35
MERGE_THRESHOLD=0.80

# CMI Relevance (council filters out non-consulting-relevant trends)
# Raised from 0.18 → 0.28: at 0.18 only 11/153 filtered (P10=0.277)
CMI_RELEVANCE_THRESHOLD=0.28
CMI_AUTO_NOISE_THRESHOLD=0.2

# Subclustering (breaks large clusters into focused sub-trends)
MAX_CHILDREN_PER_PARENT=6
MIN_ARTICLES_FOR_SUBCLUSTERING=6
MIN_SUBCLUSTER_COHERENCE=0.25

# Scraping & Dedup
SEMANTIC_DEDUP_MAX_ARTICLES=2000
SCRAPE_MAX_CONCURRENT=10

# Scoring Weights (JSON)
TREND_SCORE_WEIGHTS={"volume":0.30,"momentum":0.45,"diversity":0.25}
CONFIDENCE_SCORE_WEIGHTS={"coherence":0.28,"source_diversity":0.25,"event_agreement":0.17,"evidence_volume":0.12,"authority":0.18}
SOURCE_CREDIBILITY_WEIGHTS={"base_authority":0.40,"cross_citation":0.25,"originality":0.20,"agreement":0.15}

# Agent Thresholds
LEAD_RELEVANCE_THRESHOLD=0.3
MAX_SEARCH_QUERIES_PER_IMPACT=7
ENTERPRISE_BLOCKLIST=tata,reliance,infosys,wipro,hcl,hdfc,icici,bajaj,mahindra,adani,vedanta

# Quality Gates (V9)
# Raised from 0.3/0.25 → 0.40/0.40: prevents weak trends from triggering expensive agent calls
MIN_SYNTHESIS_CONFIDENCE=0.40
MIN_TREND_CONFIDENCE_FOR_AGENTS=0.40

# Cross-Validation (V10 Validator)
VALIDATOR_ENABLED=true
VALIDATOR_MAX_ROUNDS=2
# Raised from 0.15/0.10/0.4: all 7 clusters scored 0.43-0.54, so 0.40 pass is safe
VALIDATOR_PASS_THRESHOLD=0.40
VALIDATOR_REJECT_THRESHOLD=0.25
VALIDATOR_ENTITY_OVERLAP_MIN=0.35
VALIDATOR_WEIGHT_ENTITY=0.35
VALIDATOR_WEIGHT_KEYWORD=0.30
VALIDATOR_WEIGHT_EMBEDDING=0.35

# Synthesis Settings
SYNTHESIS_MAX_ARTICLES=16
SYNTHESIS_ARTICLE_CHAR_LIMIT=1200
SYNTHESIS_MAX_RETRIES=2
# OpenAI Configuration
# GPT-4.1-mini for generation/tools, GPT-4.1-nano for classification
# text-embedding-3-large at 1024-dim matches NVIDIA nv-embedqa-e5-v5
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4.1-mini
OPENAI_LITE_MODEL=gpt-4.1-nano
OPENAI_EMBEDDING_MODEL=text-embedding-3-large
OPENAI_EMBEDDING_DIMENSIONS=1024
